{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef09b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mahotas\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# ====== User-defined path ======\n",
    "IMDIR = \"C:/Users/prave/Final Year project/data\"  # <-- Change this to your real image folder\n",
    "\n",
    "# Setup save paths\n",
    "BOVW = \"model/bovw_codebook_600.pickle\"\n",
    "DICT_SIZE = 600\n",
    "DATA = 'model/data_600.npy'\n",
    "LABEL = 'model/label_600.npy'\n",
    "\n",
    "# ==== Feature descriptors ====\n",
    "\n",
    "# Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(gray)).flatten()\n",
    "    return feature\n",
    "\n",
    "# Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick\n",
    "\n",
    "# Color Histogram\n",
    "def fd_histogram(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    bins = 8\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "# SIFT Bag of Visual Words feature extraction\n",
    "def feature_extract(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features = bowDiction.compute(gray, sift.detect(gray))\n",
    "    if features is not None:\n",
    "        return features.flatten()\n",
    "    else:\n",
    "        # If no features detected, return zeros\n",
    "        return np.zeros(DICT_SIZE)\n",
    "\n",
    "# ======= Start Processing ========\n",
    "\n",
    "# Directory containing images\n",
    "base = Path(IMDIR).resolve()\n",
    "print(f\"Base Directory: {base}\")\n",
    "\n",
    "# Initialize SIFT and BOW trainer\n",
    "sift = cv2.SIFT_create()\n",
    "BOW = cv2.BOWKMeansTrainer(DICT_SIZE)\n",
    "\n",
    "# Build vocabulary (Bag of Words Dictionary)\n",
    "print(\"Building vocabulary...\")\n",
    "descriptor_list = []\n",
    "for file in base.glob('**/*.*'):\n",
    "    fpath = Path(file).resolve()\n",
    "    image = cv2.imread(str(fpath))\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Warning: Failed to read {fpath}\")\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    if des is not None:\n",
    "        descriptor_list.append(des)\n",
    "    else:\n",
    "        print(f\"Warning: No descriptors found for {fpath}\")\n",
    "\n",
    "# Stack all descriptors vertically\n",
    "if len(descriptor_list) == 0:\n",
    "    print(\"Error: No descriptors collected. Cannot cluster.\")\n",
    "    exit(1)\n",
    "\n",
    "descriptors = np.vstack(descriptor_list)\n",
    "print(f\"Total descriptors shape: {descriptors.shape}\")\n",
    "\n",
    "# Add descriptors to BOW trainer and cluster\n",
    "BOW.add(descriptors)\n",
    "print(\"Clustering...\")\n",
    "dictionary = BOW.cluster()\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")\n",
    "\n",
    "# Save the codebook\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "with open(BOVW, \"wb\") as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "print(f\"Codebook saved to {BOVW}\")\n",
    "\n",
    "# Load the codebook again\n",
    "with open(BOVW, \"rb\") as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "# Initialize BOW image descriptor extractor\n",
    "sift2 = cv2.SIFT_create()\n",
    "bowDiction = cv2.BOWImgDescriptorExtractor(sift2, cv2.BFMatcher(cv2.NORM_L2))\n",
    "bowDiction.setVocabulary(dictionary)\n",
    "\n",
    "# ===== Feature Extraction =====\n",
    "print(\"Feature Extraction...\")\n",
    "\n",
    "x_data = []\n",
    "x_label = []\n",
    "\n",
    "for file in base.glob('**/*.*'):\n",
    "    fpath = Path(file).resolve()\n",
    "    image = cv2.imread(str(fpath))\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Warning: Failed to read {fpath}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        Humo = fd_hu_moments(image)\n",
    "        Harl = fd_haralick(image)\n",
    "        Hist = fd_histogram(image)\n",
    "        Bovw = feature_extract(image)\n",
    "\n",
    "        mfeature = np.hstack([Humo, Harl, Hist, Bovw])\n",
    "        x_data.append(mfeature)\n",
    "        x_label.append(int(fpath.parent.name))  # Folder name as label\n",
    "        print(f\"Processed {fpath.name}, feature shape: {mfeature.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fpath.name}: {e}\")\n",
    "\n",
    "# ===== Save the final datasets =====\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "x_label = encoder.fit_transform(x_label)\n",
    "\n",
    "# Save the data and labels\n",
    "np.save(DATA, np.array(x_data))\n",
    "np.save(LABEL, np.array(x_label))\n",
    "\n",
    "print(f\"Data saved to {DATA}\")\n",
    "print(f\"Labels saved to {LABEL}\")\n",
    "\n",
    "print(\"âœ… Completed Successfully!\")\n",
    "\n",
    "'''\n",
    "Sample Run:\n",
    "Just run this script. No command line arguments needed.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import os\n",
    "\n",
    "# Configure file paths\n",
    "DATA = 'model/data_600.npy'\n",
    "LABEL = 'model/label_600.npy'\n",
    "MODEL_PATH = 'model/rf_model.pkl'\n",
    "\n",
    "# Load data and labels\n",
    "data = np.load(DATA)\n",
    "label = np.load(LABEL)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize Random Forest\n",
    "print(\"\\n<==== Model ====>\\n\")\n",
    "clf = RandomForestClassifier(n_jobs=-1, verbose=1, random_state=42)\n",
    "print(clf)\n",
    "\n",
    "print(\"\\n<==== Hyperparameter Search ====>\\n\")\n",
    "\n",
    "# Define new, optimized hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],  # Less large estimators\n",
    "    'max_depth': [10, 15, 20, 25, None],  # More moderate tree depths\n",
    "    'min_samples_split': [2, 5, 10],  # More common split values\n",
    "    'min_samples_leaf': [1, 2, 4],  # More common leaf values\n",
    "    'max_features': ['sqrt', 'log2', None]  # Different ways to handle features\n",
    "}\n",
    "\n",
    "# Randomized Search (Use this to avoid exhaustive search)\n",
    "random_search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Adjusted for fewer iterations (still 20 combinations)\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "best_model = random_search.fit(x_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"\\n<==== Best Model ====>\\n\")\n",
    "print(\"Best Score (Training):\", best_model.best_score_)\n",
    "print(\"Best Parameters:\", best_model.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(x_test)\n",
    "print(\"\\n<==== Test Accuracy ====>\\n\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n<==== Confusion Matrix ====>\\n\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n<==== Classification Report ====>\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model\n",
    "print(\"\\n<==== Saving Model ====>\\n\")\n",
    "os.makedirs('model', exist_ok=True)\n",
    "dump(best_model, MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "'''\n",
    "Sample run:\n",
    "$ python hyper_train.py\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e55571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib  # Import joblib directly\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "# Set model file path manually for Jupyter or direct execution\n",
    "MODEL = 'model/rfclassifier_600.sav'  # Example fixed file path\n",
    "DATA = 'model/data_600.npy'\n",
    "LABEL = 'model/label_600.npy'\n",
    "\n",
    "# Load data and labels\n",
    "data = np.load(DATA)\n",
    "label = np.load(LABEL)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize a random forest classifier\n",
    "print(\"\\n<==== Model ====>\\n\")\n",
    "clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, verbose=1)\n",
    "print(clf)\n",
    "\n",
    "print(\"\\n<==== Training ====>\\n\")\n",
    "\n",
    "# Train the model\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Check accuracy on test data\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "print(\"\\n<==== Confusion Matrix ====>\\n\")\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(cf)\n",
    "\n",
    "print(\"\\n<==== Cross-validation ====>\\n\")\n",
    "\n",
    "# Evaluate the model using five-fold cross-validation\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5)  \n",
    "print(\"\\nAccuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, MODEL)\n",
    "\n",
    "print(f\"\\nModel saved to: {MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mahotas\n",
    "import pickle\n",
    "import joblib  # Corrected import for joblib\n",
    "\n",
    "# Configure file paths and parameters\n",
    "IMAGE = 'test/2000.jpg'  # Change this if using Jupyter or set via sys.argv[1]\n",
    "BOVW = \"model/bovw_codebook_600.pickle\"\n",
    "MODEL = 'model/rfclassifier_600.sav'\n",
    "IMG_SIZE = 320\n",
    "\n",
    "# Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature\n",
    "\n",
    "# Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick\n",
    "\n",
    "# Color Histogram\n",
    "def fd_histogram(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    bins = 8\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "# SIFT Bag of Visual Words\n",
    "def feature_extract(im, bowDiction):\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()  # Updated to SIFT_create() for OpenCV 4.4.0+\n",
    "    kp = sift.detect(gray)\n",
    "    feature = bowDiction.compute(gray, kp)\n",
    "    return feature.squeeze()\n",
    "\n",
    "# Load the trained model and input image\n",
    "loaded_model = joblib.load(MODEL)  # joblib import\n",
    "image = cv2.imread(IMAGE)\n",
    "\n",
    "# Resize the image\n",
    "(height, width, channel) = image.shape\n",
    "resize_ratio = 1.0 * (IMG_SIZE / max(width, height))\n",
    "target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "input_image = cv2.resize(image, target_size)\n",
    "\n",
    "cv2.imwrite(\"res_img.png\", input_image)\n",
    "\n",
    "# Class-label dictionary\n",
    "label = {0: \"10\", 1: \"20\", 2: \"50\", 3: \"100\", 4: \"200\", 5: \"500\", 6: \"2000\"}\n",
    "\n",
    "# Load the BOVW codebook\n",
    "with open(BOVW, \"rb\") as pickle_in:\n",
    "    dictionary = pickle.load(pickle_in)\n",
    "\n",
    "# Initialize SIFT BOW image descriptor extractor\n",
    "sift2 = cv2.SIFT_create()  # Updated to SIFT_create() for OpenCV 4.4.0+\n",
    "bowDiction = cv2.BOWImgDescriptorExtractor(sift2, cv2.BFMatcher(cv2.NORM_L2))\n",
    "bowDiction.setVocabulary(dictionary)\n",
    "\n",
    "# Extract the features\n",
    "Hu = fd_hu_moments(input_image)\n",
    "Harl = fd_haralick(input_image)\n",
    "Hist = fd_histogram(input_image)\n",
    "Bovw = feature_extract(input_image, bowDiction)\n",
    "\n",
    "# Generate a feature vector by combining all features\n",
    "mfeature = np.hstack([Hu, Harl, Hist, Bovw])\n",
    "\n",
    "# Predict the output using the trained model\n",
    "output = loaded_model.predict(mfeature.reshape((1, -1)))\n",
    "print(\"\\nPredicted class: \" + label[output[0]])\n",
    "\n",
    "'''\n",
    "Sample run: python predict.py test/2000.jpg\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mahotas\n",
    "import imutils\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import pyttsx3  # for speech output\n",
    "\n",
    "# Setup text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # Speed of speech\n",
    "\n",
    "# Load trained model\n",
    "MODEL = 'model/rfclassifier_600.sav'\n",
    "BOVW = \"model/bovw_codebook_600.pickle\"\n",
    "loaded_model = joblib.load(MODEL)\n",
    "\n",
    "# Load BOVW codebook\n",
    "with open(BOVW, \"rb\") as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "# Initialize SIFT BOW descriptor\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "bowDiction = cv2.BOWImgDescriptorExtractor(sift, cv2.BFMatcher(cv2.NORM_L2))\n",
    "bowDiction.setVocabulary(dictionary)\n",
    "\n",
    "# Label dictionary\n",
    "label = {0: \"10\", 1: \"20\", 2: \"50\", 3: \"100\", 4: \"200\", 5: \"500\", 6: \"2000\"}\n",
    "\n",
    "# Define feature extraction functions\n",
    "def fd_hu_moments(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(gray)).flatten()\n",
    "    return feature\n",
    "\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick\n",
    "\n",
    "def fd_histogram(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    bins = 8\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def feature_extract(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift_features = bowDiction.compute(gray, sift.detect(gray))\n",
    "    return sift_features.squeeze()\n",
    "\n",
    "def extract_features(image):\n",
    "    humo = fd_hu_moments(image)\n",
    "    harl = fd_haralick(image)\n",
    "    hist = fd_histogram(image)\n",
    "    bovw = feature_extract(image)\n",
    "    combined = np.hstack([humo, harl, hist, bovw])\n",
    "    return combined\n",
    "\n",
    "# Function to detect denomination\n",
    "def detect_denomination(img):\n",
    "    img_size = 320\n",
    "    (height, width, _) = img.shape\n",
    "    resize_ratio = 1.0 * (img_size / max(width, height))\n",
    "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "    \n",
    "    img = cv2.resize(img, target_size)\n",
    "\n",
    "    features = extract_features(img).reshape(1, -1)\n",
    "    prediction = loaded_model.predict(features)[0]\n",
    "    \n",
    "    denomination = label[prediction]\n",
    "    print(f\"Detected denomination: â‚¹{denomination}\")\n",
    "    \n",
    "    # Speak out\n",
    "    engine.say(f\"This is a {denomination} rupees note\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    return denomination\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press SPACEBAR to capture and detect denomination.\")\n",
    "print(\"Press ESC to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the webcam feed\n",
    "    cv2.imshow('Currency Detection', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC to exit\n",
    "        break\n",
    "    elif key == 32:  # SPACEBAR pressed\n",
    "        detect_denomination(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e991db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
